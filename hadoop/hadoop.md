Hadoop is an open source framework managed by the Apache Software Foundation. Open source refers to the fact that it is freely available, and its source code can be changed in accordance with user requirements. Apache Hadoop is designed to store and process big data effectively. It can handle various forms of structured and unstructured data and provides enough flexibility for collecting, processing and analyzing data.

Hadoop includes 2 main systems. First is Hadoop Distributed File System (HDFS), which is the storage system for Hadoop spread out over multiple machines. The second one is the MapReduce engine—the algorithm that filters, sorts and uses the database input in some way. HDFS is very closely coupled with MapReduce so data from HDFS is transferred to MapReduce for further processing. When HDFS receives data, it splits this information into separate blocks and allocates them to different nodes in a cluster which allows it to perform tasks in parallel and work more efficiently.

Amazon EMR and GCP Dataproc are managed Hadoop and Spark services, which are generally used for big data processing, streaming, interactive analysis and machine learning. EMR and Dataproc clusters can be created with many of the popular Apache Hadoop ecosystem components installed. They support the installation and configuration of these applications. Some of the applications include Apache Spark, Apache Hadoop, Apache Pig, Apache Hive and Presto.

In fact, Apache Hadoop is not a distribution because each component can be downloaded separately. Native Hadoop deployment, “Vanilla" Hadoop is provided by the Apache Software Foundation. It is 100% community-driven. Binary versions of Hadoop components are available on the website for each of the ecosystem projects. Commonly, utilizing a vanilla version of Apache Hadoop equals to rolling your own Linux distribution.
However, tying everything together is labor-intensive. Companies bundle Hadoop with open source, their own proprietary bricks and other tools to design distributions.

## What is Hadoop?

The Apache Hadoop Software library is a framework that provides an opportunity for distributed processing of huge sets of data across computer clusters through simple programming models. The most attractive feature of Apache Hadoop is that it is open source software under Apache License 2.0. In simple words, the framework is free to use. Therefore, anyone can download and use it personally or professionally.

The programming language the Hadoop framework is written in is Java in combination with some native code in C. Any programming language can be used with Hadoop Streaming (for example, Ruby, Python) and Hadoop Pipes (C++) to implement the main features of this framework. There are various tools for various purposes. Hadoop framework has a wide range of tools. The framework is divided into two layers: the storage layer and the processing layer. The Hadoop Distributed File System stands for the storage layer and Map Reduce stands for the processing layer.

### HDFS
Hadoop Distributed File System (HDFS) is a primary storage system of Hadoop. HDFS stores very large files, running as a sequence of blocks on a cluster of commodity hardware. Except for the last block, all the rest are of the same size. It follows the principle of storing a low number of large files rather than a huge number of small files. HDFS is able to keep data securely even if hardware fails. Furthermore, it ensures high throughput access to the application through accessing in parallel.

### MapReduce

MapReduce is an associated implementation and a programing pattern in the Hadoop framework for collecting and processing big sets of data with a distributed, parallel algorithm on a cluster. The program consists of a map procedure, which is responsible for sorting and filtering, and a reduce method, which does a summary operation. The "MapReduce System" (also called "infrastructure" or "framework") arranges the processing by running the multiple tasks simultaneously in parallel, marshaling the distributed servers, handling all data transfers and communications between the different parts of the system, and providing for fault and redundancy tolerance.

## Hadoop Architecture Fundamentals

NameNode is a master server that exists in the HDFS cluster. It manages the file system namespace by executing an operation like the opening, renaming and closing files. Also, it simplifies the architecture of the system. A NameNode handles the file system namespace and regulates access to files by clients. Client applications communicate to the NameNode at any time they need to locate a file, or when the file should be added, copied, moved or deleted. The NameNode responds back with the successful requests by returning a list of relevant DataNode servers where the data lives.

DataNode represents the second layer of Hadoop architecture. A DataNode manages the state of an HDFS node and interacts with the blocks. It can perform jobs like semantic and language analysis, statistics and machine learning tasks, and also jobs like clustering, data import, data export, search, decompression and indexing. On startup, every DataNode connects to the NameNode and performs a handshake to verify the namespace ID and the software version of the DataNode. The DataNode verifies the block replicas in its ownership by sending a block report to the NameNode. The DataNode sends a heartbeat to the NameNode every 3 seconds to confirm that the DataNode is operating and the block replicas it hosts are available.

ARN (Yet Another Resource Negotiator) was added to Hadoop 2.0 with the aim to remove the bottleneck on Job Tracker, which was present in Hadoop 1.0. YARN is currently known as a large-scale distributed operating system used for Big Data processing. Also, YARN allows different data processing methods like graph processing, interactive processing, stream processing as well as batch processing to run and process data stored in HDFS. Thereby, the YARN component opens the Apache Hadoop software library to different distributed applications beyond MapReduce. Moreover, in addition to resource management, Yarn performs the scheduling function.

Metastore is the component whose main function is to store all the structure data of different tables and partitions in a warehouse including serializers and deserializers required to write and read data, columns and column type information, and relevant HDFS files which store the data. Metastore is the component that keeps all the structure data of the different partitions and tables in a data warehouse including column and column type information, the serializers and deserializers required to read and write data and the appropriate HDFS files where the data is stored. Also, Metastore offers two critical but frequently neglected features of the data warehouse: data discovery and data abstraction.
Without the data abstractions, a user has to provide information about data formats, extractors and loaders along with the query. At the same time, data discovery enables users to discover and explore relevant and specific data in the warehouse.

Hadoop Architecture comprises three major layers: HDFS (Hadoop Distributed File System), Yarn and MapReduce. HDFS provides for data storage of Hadoop; MapReduce is the data processing layer of Hadoop; and YARN or Yet Another Resource Negotiator is the resource management layer of Hadoop. In Hadoop architecture, one masternode and multiple datanodes are available. As was already mentioned, masternode’s function is to assign a task to various datanodes and manage resources. The datanodes do actual computing. They store the real data whereas on master we have metadata. This means it stores data about data.

The primary objective of HDFS is to store data reliably even in the presence of failures including NameNode failures, DataNode failures and network partitions. HDFS architecture includes NameNode, multiple Datanodes and blocks.
DataNode and NameNode are pieces of software created for running on commodity computers. Commonly, the user data is held in the HDFS files. The file in a file system will be split into one or more segments and/or stored in separate datanodes. These segments of a file are also known as blocks. That is to say, the smallest volume of data that HDFS can write or read is named a Block. The default size of one block is 128 MB.

Parallel computing is computing architecture where many independent operations are executed simultaneously in any order. Parallel processing allows for making quick work on a big data set because instead of having one processor doing all the work, you split up the task amongst many processors. Hadoop’s technology enables parallel processing, which enables parallel searching, metadata management, parallel analysis (with MapReduce) and the establishment of workflow system analysis. In particular, MapReduce provides a specific programming “template” for parallel analysis of large-scale data sets. Thus, a MapReduce job can be executed in parallel across thousands of nodes in a cluster.

An RDBMS works well with structured data while the Hadoop software framework provides opportunities for effective analysis of non-structured, semi-structured or structured data. Hadoop can manage and analyze bigger amounts of data than RDBMS. RDBMS works better when the volume of data is low (in Gigabytes). However, when the data size is evaluated Petabytes, Hadoop is the best solution. What is more, Hadoop has higher throughput. RDBMS provides vertical scalability, which is also called "Scaling Up" a machine, which means you can add more resources or hardware, such as memory or CPU, to a machine in the computer cluster. While Hadoop provides horizontal scalability, which is known as "Scaling Out" a machine and means adding more machines to the existing computer clusters as a result of which Hadoop becomes a fault-tolerant.

Secondary NameNode serves a crucial role in managing the filesystem metadata, but contrary to what its name might suggest, it does not act as a failover for the primary NameNode. Instead, its primary function is to help maintain the health of the Hadoop Distributed File System (HDFS) by periodically merging the edit logs from the NameNode with the filesystem image, thus preventing these logs from growing indefinitely. This process, known as checkpointing, involves fetching the current filesystem image and edit logs from the NameNode, merging them locally, and then returning a new, compacted image back to the NameNode. This operation helps in optimizing the performance of the NameNode and ensures that the filesystem metadata does not become a bottleneck due to excessive size of the edit logs.

In High Availability (HA) configuration, the Standby NameNode plays a critical role as a backup to the Active NameNode, ensuring continuous availability of the Hadoop Distributed File System (HDFS). Unlike the Secondary NameNode, the Standby NameNode is an active participant in the filesystem's metadata operations. It maintains a hot standby state by staying synchronized with the Active NameNode through a dedicated communication protocol that continuously mirrors all changes made to the filesystem metadata (edit logs). In the event of an Active NameNode failure, the Standby NameNode is immediately ready to take over its responsibilities without data loss or significant downtime, thus providing a seamless failover mechanism and enhancing the resilience and reliability of the HDFS.

Small files are the major problem in HDFS. A small file is significantly smaller than the HDFS block size (default: 128MB). If instead of big files you're not keeping small ones, then, most likely, there are thousands of them. However, the dilemma is that HDFS is unable to manage a huge number. As far as NameNode holds the file system image in the memory, it has more data to store with small files, not with large ones. Moreover, it has to do a lot of work while handling FSImages and edit logs because more files should be written.

## HDFS Main Terms Explained

A block is the smallest unit of storage on a computer system, representing the minimum amount of contiguous storage space allocated to a file. In Hadoop, the default block size is typically set at 128MB or 256MB, and choosing the appropriate block size is crucial. For instance, consider a 700MB file: with a block size of 128MB, HDFS would split this file into six blocks—five blocks of 128MB each and one block of 60MB. If the block size were only 4KB, the file would be divided into an excessively large number of blocks. This would result in a substantial amount of metadata, potentially overwhelming the NameNode due to the sheer volume of blocks to manage, especially considering that HDFS handles files ranging from terabytes to petabytes. Therefore, selecting an optimal block size for HDFS is essential to maintain efficiency and prevent overloading the NameNode.

Replication in HDFS refers to the process of storing multiple copies of data blocks across different nodes in a Hadoop cluster to ensure data reliability and availability. This redundancy allows the system to continue functioning seamlessly even if some nodes fail. The figure depicted illustrates the functioning of the replication technique. Consider a scenario where there is a 1GB file; with a replication factor of 3, this would necessitate a total of 3GB of storage space.

To uphold this replication factor, the NameNode gathers block reports from each DataNode. Based on these reports, if a block is found to be under-replicated or over-replicated, the NameNode will either create additional replicas or remove excess ones to maintain the desired replication level.

Rack awareness is a strategy used in Hadoop to improve data reliability and network performance by considering the physical layout of the server racks when distributing data. In a Hadoop cluster, data is not only replicated across different nodes but is also strategically placed across different racks. This approach ensures that if an entire rack fails due to power, network, or hardware issues, the data remains available on another rack, thereby enhancing fault tolerance. Additionally, rack awareness optimizes data retrieval by minimizing network traffic between racks, which can be slower and more costly compared to traffic within the same rack. By understanding the rack topology, Hadoop can intelligently place data to balance load and reduce inter-rack data transfer, leading to improved performance and reduced latency.

Data locality in HDFS refers to the strategy of minimizing network congestion and increasing the overall throughput of the system by processing data on the node where the data is stored. When a Hadoop job is executed, the Hadoop framework tries to assign tasks to nodes where the data blocks needed for the task are already present. This approach reduces the need for data to be transferred across the network to different nodes, which can be time-consuming and resource-intensive. By executing tasks on nodes where data is locally available, Hadoop significantly speeds up processing times and enhances the efficiency of the cluster. This concept is particularly beneficial in large-scale environments, where data transfer between nodes can become a major bottleneck.

## Hbase

It is a no SQL database that runs on top of Hadoop. It is a database that stores structured data in tables that could have billions of rows and millions of columns. H-Base also provides Real-Time access to read or write data in HDFS. The components of Hbase are H-Base Master, Region server, AV master, which is not part of the actual data storage, but it just performs administration like interface for creating, updating and deleting tables. The region server is the worker node—it handles reads, writes, updates and deletes requests from clients region server. Also, the process works on all nodes in the Hadoop cluster.
